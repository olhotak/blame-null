\section{Introduction}

Null pointers are infamous for causing software errors.
\citet{Hoare} characterised them as ``The Billion Dollar Bug''.

One way to tame the danger of nulls is via types.  Whereas older
languages, such as Pascal and Java, permit nulls at any reference
type, more recent designs, including Kotlin, Scala, C\#, and Swift,
adopt type systems that track whether a reference may be null.  How do
we permit code in older and newer languages to interact while
preserving the type guarantees of the newer languages?

Gradual typing provides a sound theoretical basis for answering such
questions, where a legacy language with a less precise type system
(such as Java) interacts with a newer language with a more precise
type system (such as Kotlin or Scala).  Important early systems
include those by \citet{Findler-and-Felleisen} and
\citet{Siek-and-Taha}.  They introduce casts to model monitoring the
barrier between the two languages.  Each cast checks at runtime
whether values passed from the less-precisely typed language violate
guarantees expected by more-precisely typed language.

A key innovation, introduced by \citet{Findler-and-Felleisen} is that
when a cast fails blame is attributed to either the source or the
target of the cast.  \citet{Greenberg-and-Felleisen} [TODO: check
citation!] exploit this innovation to prove that when a cast fails,
blame always lies with the less-precisely typed side of the cast.
Though the fact is obvious, their proof is not, depending on
observational equivalence.  \citet{Wadler-and-Findler} introduced
the \emph{blame calculus} as an abstraction of the earlier models, and
offered a simpler proof of the obvious fact based on a simple
syntactic notion of \emph{blame safety} and a straightforward
proof basedd on progress and preservation.

\citet{Nieto-et-al-2019} applied gradual typing and blame to the
case of type systems that track null references.
Their $\lambdanull$ calculus supports three function types:
\begin{itemize}
\item $\#(S \to T)$ is a non-nullable function, corresponding to a
  non-nullable type such as \code{String} in Scala or Kotlin.
  Values of this type cannot be null. (There are technical exceptions
  where the value can be null, as explained in that paper.)
\item $?(S \to T)$ is a safe nullable function, corresponding to
  a nullable type such as \code{String|Null} in Scala or
  \code{String?} in Kotlin.  Values of this type can be null,
  and the type system ensures nulls are properly handled.
\item $!(S \to T)$ is an unsafe nullable function, corresponding
  to a type such as \code{String} in Java. Values of this type
  can be null, but the type system guarantees nothing about proper
  handling of such nulls.
\end{itemize}

Their system also supports two forms of application,
normal application $\app{s}{t}$
and safe application $\safeapp{s}{t}{u}$.
Both apply $s$ to $t$ when the function is not null,
but when the function is null the former gets stuck
while the latter returns $u$.  The two forms of application
align with the three function types as follows.
Consider the type of a function term $s$.
\begin{itemize}
\item $\#(S \to T)$ can be applied using standard application $\app{s}{t}$.
\item $?(S \to T)$ can be applied using safe application $\safeapp{s}{t}{u}$.
\item $!(S \to T)$ can be applied using either standard application
  $\app{s}{t}$ or safe application $\safeapp{s}{t}{u}$.
\end{itemize}

Casts may be used to convert the types of terms, and in particular
to convert functions between these various types.  At runtime, if
a cast attempts to convert null from one of the latter two types
to the first type the cast will fail, assigning blame appropriately
to one side or the other of the cast.

On top of $\lambdanull$, that paper also defines $\lambdanulls$, a
calculus representing two languages, one with nulls reflected
explicitly in its types (like Scala or Kotlin) and one where nulls are
implicitly permitted everywhere (like Java).  The syntax of the two
languages is mutually recursive with an import construct that makes it
possible to embed a term of one of the languages within a term of the
other, modeling that it is possible to call either language from the
other.  The typing rules require each such embedded term to be closed,
so it cannot have free variables bound in the other language.  Thus it
is not possible to construct heterogeneous data structures, such as a
closure that closes over bindings from the other language. [TODO:
Check!]  The semantics of $\lambdanulls$ is defined by translation to
$\lambdanull$, with import constructs translated to
corresponding casts.  The key result is that if any of these casts
fails, the blame is always assigned to code from the less-precisely
typed implicit language.

This paper reiterates the development of the earlier paper,
but using a simpler system and one that is closer to the standard
development of blame calculus.
\begin{itemize}
\item  Instead of three variants of function types,
  our design is more orthogonal.  There is a function type $A \to B$,
  and there is a nullable type $D?$, which adds nulls to an existing
  type $D$.  Here $A$ and $B$ range
  over all types, while $D$ is restricted to \emph{definite} types that do not
  already admit nulls.  (This syntax rules out potentially confusing
  types such as $D??$.)  The values of type $D?$ are either \code{null}
  or of the form $[V]$, where $V$ is a value of type $D$.

\item Instead of two forms of application, one
  safe and one unsafe, our orthogonal system of types leads to
  a corresponding orthogonal system of terms, based on
  standard forms of application for functions and case
  analysis for nullable values.

\item Instead of a high-level language $\lambdanulls$ with explicit
  and implicit sublanguages that translates into a core language
  $\lambdanull$, we use a simpler framework. We define an
  \emph{explicit} language that fills the roll of both $\lambdanull$
  and the explicit half of $\lambdanulls$ and we define an
  \emph{implicit} language that is given a semantics by translation
  into the explicit language.
  
\item Because the implicit language is given a semantics by
  translation into the explicit language, it is easy to assign
  a semantics to arbitrary nesting of explicit and implicit
  terms.  There is no longer a requirement that nested terms
  be closed; free variables of a term in one language can
  be bound in the other language.

\item The resulting development is simpler and more standard than
  the previous development.  In particular, we adapt the Tangram
  Lemma of \citet{Wadler-and-Findler} to prove that blame is
  always assigned to the less-precisely typed language.
  The previous development never mentioned the Tangram Lemma,
  relying instead on a more convoluted argument.
\end{itemize}  
Thus, our system can serve as a simpler and
more canonical foundation for formal models of the interaction
between languages with explicit and implicit nulls.

The paper is organised as follows.
Section~\ref{sec:defn} defines the explicit language.
Section~\ref{sec:thm} proves its key properties:
type safety, blame safety, and the Tangram Lemma.
Section~\ref{sec:impdefn} defines the implicit language
and its translation to the explicit language,
and proves that the translation preserves types.
Section~\ref{sec:interop} explores interoperability of
the two languages: we show how terms
of each language can be embedded in the other, define the casts
needed to mediate between the two, and prove that any failure of
these casts always blames the implicit language.
Section~\ref{sec:rw} surveys related work.
Section~\ref{sec:conclusion} concludes.

We have formalized all of our lemmas and theorems in Coq. We will
submit our Coq formalization to the OOPSLA Artifact Evaluation
process.
